{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Miniproject 2: Chatbot\n",
    "\n",
    "## Introduction\n",
    "\n",
    "### Description\n",
    "\n",
    "Developing a model employing ANN on real-world data requires going through several major steps, each of which with \n",
    "important design choices that directly impact the final results. \n",
    "In this project, we guide you through these choices starting from a large database of \n",
    "[conversations](http://parl.ai/downloads/personachat/personachat.tgz) to a functional chatbot. \n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- You should have a running installation of [tensorflow](https://www.tensorflow.org/install/) and [keras](https://keras.io/).\n",
    "- You should know the concepts \"recurrent neural networks\", \"LSTM\", \"training and validation data\", \"overfitting\" and \"early stopping\".\n",
    "\n",
    "### What you will learn\n",
    "\n",
    "- You will be guided through a data processing procedure and understand the importance of design choices in ANN modeling\n",
    "- You will learn how to define recurrent neural networks in keras and fit them to data.\n",
    "- You will be guided through a prototyping procedure for the application of deep learning to a specific domain.\n",
    "- You will get in contact with concepts discussed in the lecture, like \"overfitting\", \"LSTM network\", and \"Generative model\".\n",
    "- You will learn to be more patient :) Some fits may take your computer quite a bit of time; run them over night and make sure you save (and load) your data and models.\n",
    "\n",
    "### Evaluation criteria\n",
    "\n",
    "The evaluation is (mostly) based on the figures you submit and your answer sentences. \n",
    "We will only do random tests of your code and not re-run the full notebook. \n",
    "Please ensure that your notebook is fully executed before handing it in. \n",
    "\n",
    "### Submission \n",
    "\n",
    "You should submit your notebook through the Moodle page submission tool. You should work in teams of two people and each member should submit the same notebook to Moodle.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions and imports\n",
    "\n",
    "For your convenience we import some libraries and provide some functions below. Fill in your names, sciper numbers and run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "names = {'student_1': \"Andrea Piccione\",\n",
    "        'student_2': \"Marshall Cooper\"}\n",
    "\n",
    "sciper = {'student_1': 294045, \n",
    "          'student_2': 299656}\n",
    "\n",
    "seed = sciper['student_1']+sciper['student_2']\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os, sys\n",
    "import copy\n",
    "\n",
    "plt.rcParams['font.size'] = 28\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams[\"axes.grid\"] = False\n",
    "c = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "import keras\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Masking, TimeDistributed, Dense, Concatenate, Dropout, LSTM, GRU, SimpleRNN, Bidirectional, Embedding, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "## TODO: Fix the datapath for final submission!\n",
    "def getRawDataFromFile(datapath=\"../data/personachat/\", file=\"train_both_revised.txt\"):\n",
    "    \n",
    "    f = open(datapath+file)\n",
    "\n",
    "    conversations = []\n",
    "    current_conversation = []\n",
    "    \n",
    "    for l, line in enumerate(f):\n",
    "        #print(l, line)\n",
    "        if \"persona:\" in line:\n",
    "            if len(current_conversation) > 1:\n",
    "                conversations.append(current_conversation)\n",
    "            current_conversation = [] \n",
    "            continue\n",
    "\n",
    "        #remove numberings\n",
    "        processed_line = line.split(' ')\n",
    "        processed_line = \" \".join(processed_line[1:])\n",
    "        line = processed_line\n",
    "        #print(line)\n",
    "\n",
    "        conv = line.split('\\t')    \n",
    "        q = conv[0]\n",
    "        a = conv[1]\n",
    "        current_conversation.append(q)\n",
    "        current_conversation.append(a)\n",
    "    \n",
    "    return conversations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data visualization and preprocessing\n",
    "\n",
    "Here we will process and visualize the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse raw data \n",
    "\n",
    "Download the dataset on http://parl.ai/downloads/personachat/personachat.tgz. Unpack it and add it to your project folder. Read and run the getRawDataFromFile function (if needed, modify the default path). It extracts the conversations.\n",
    "\n",
    "**Output** Display two randomly selected conversations. [1 pt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations = getRawDataFromFile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['hi . i love catching sunfish and am a simple guy',\n",
       "  'hi how are you today',\n",
       "  'fish are pretty but unfortunately i cant eat them',\n",
       "  'why can you not eat them',\n",
       "  'that is a good question . but i do like beer and creeks .',\n",
       "  'do you drink them at the creeks',\n",
       "  'yes . and fish there . but i do not know why',\n",
       "  'are you made of fish then',\n",
       "  'haha . no . i do not think so . but i think they are pretty',\n",
       "  'yeah they have great scales',\n",
       "  'yes ! my chihuahua is stubborn',\n",
       "  'have to discipline those dogs',\n",
       "  'they are little boogers ! stinkin dogs',\n",
       "  'i work at the zoo though'],\n",
       " ['hello , how are you ? where do you live ? i am in california .',\n",
       "  'hi i am mad about work but will be ok how are you',\n",
       "  'i am fine . sorry to hear that you are upset , what is bothering you ?',\n",
       "  'been working at the factory 10 years but my coworker gets promoted',\n",
       "  'how long was your coworker working there ?',\n",
       "  '5 years . if she had been there longer i would have understood it',\n",
       "  'did she show more effort than you did ? what kind of work ?',\n",
       "  'nope but i am not working overtime or anything anymore forget that .',\n",
       "  'i am sorry . would you like to talk about another topic ?',\n",
       "  'what do you do for a living',\n",
       "  'i work at a nuclear power plant . what do you make at the factory ?',\n",
       "  'i make little debbie cakes',\n",
       "  'those are yummy ! i also like spaghetti !',\n",
       "  'spaghetti is great and quick and easy to make',\n",
       "  'that may be why i like it ! do you do anything for fun ?',\n",
       "  'i like to play videogames']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[conversations[i] for i in np.random.randint(0, len(conversations),size=2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract word tokens\n",
    "\n",
    "Let's start looking at our data. \n",
    "\n",
    "**Code** Compute the set of unique words (dictionary) in all sentences along with the number of occurences of each of these words. HINT: each word is separated by a space character, use the python string.split(' ') function to separate words. Consider punctuations as 'words'. [1 pt]\n",
    "\n",
    "**Figure** In a bar plot, show the first 75 most frequent words (x-axis) and their number of occurences (y-axis). [1 pt]\n",
    "\n",
    "**Figure** In another bar plot, show the 75 least frequent words (x-axis) and their number of occurences (y-axis). [1 pt] \n",
    "\n",
    "**Figure** In a log-log scale, plot the sorted word index (x-axis) vs their respective count (y-axis). [1 pt]\n",
    "\n",
    "**Question** Relate the sorted word count distribution with Zipf's law.\n",
    "Argue using the log-log plot. [1 pt]\n",
    "\n",
    "**Answer**\n",
    "\n",
    "**Question** How many words appear only once in the entire dataset? [1 pt]\n",
    "\n",
    "**Answer**\n",
    "7080\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Compute set of unique words\n",
    "from collections import Counter\n",
    "words = Counter()\n",
    "for conv in conversations:\n",
    "    for dialogues in conv:\n",
    "        words.update(dialogues.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7080"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([word for word in words if words[word]==1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering\n",
    "\n",
    "We suggest to filter your data by removing sentences containing rare words. \n",
    "\n",
    "\n",
    "**Code** To achieve that, you should create a new dataset where sentences containing rare words (words that occur less than N times in the dataset) are removed. Keep at least 50'000 sentences (depending on your computing power, you can keep more). \n",
    "HINT: Start by updating the dictionary accordingly and then remove any sentence that contains at least a single word that is not in the dictionary of words. [2 pts]\n",
    "\n",
    "**Question**: How much did you reduce the number of unique words with your rare event suppression procedure? [1 pt]\n",
    "    \n",
    "**Answer**: 18673 words were pruned -> 1007 words, approximately 94% reduction.\n",
    "\n",
    "**Question**: How many sentences are in your filtered and original dataset? [1 pt]\n",
    "\n",
    "**Answer**: 131424 sentences -> 50793 sentences including only words that appear more than 100 times.\n",
    "\n",
    "**Question**: What is the impact on learning and generalization of removing sentences with rare words from your dataset? [2 pt]\n",
    "\n",
    "**Answer**: TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering N = 50 : 66500 sentences remaining N.\n",
      "Filtering N = 100 : 50793 sentences remaining N.\n",
      "Filtering N = 150 : 41527 sentences remaining N.\n",
      "Using N = 100\n",
      "18673 words originally, reduced to 1007: 94% reduction.\n"
     ]
    }
   ],
   "source": [
    "# Declare N and threshold of remaining sentences\n",
    "threshold = 50000\n",
    "# Update dictionary\n",
    "filtered = conversations\n",
    "N = 0\n",
    "prev = filtered\n",
    "pruned = dict([(i, words[i]) for i in words if words[i]>N])\n",
    "total = sum([len(i) for i in filtered])\n",
    "while(total>threshold):\n",
    "    prev = (filtered, pruned, N)\n",
    "    N += 50\n",
    "    pruned = dict([(i, words[i]) for i in words if words[i]>N])\n",
    "    filtered = [i for i in [[sentence for sentence in conversation if all([word in pruned.keys() for word in sentence.split(' ')])] for conversation in conversations] if i!=[]]\n",
    "    total = sum([len(i) for i in filtered])\n",
    "    print(\"Filtering N = {0} : {1} sentences remaining N.\".format(N, total))\n",
    "\n",
    "    \n",
    "filtered, pruned, N = prev\n",
    "total = sum([len(i) for i in filtered])\n",
    "print(\"Using N = {0}\".format(N))\n",
    "print(\"{0} words originally, reduced to {1}: {2}% reduction.\".format(len(words), len(pruned), int(100*(1-(len(pruned)/len(words))))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization and padding\n",
    "\n",
    "Now you will transform our filtered data into a format that is understandable by an ANN. To achieve that, you should transform words to integers, where single integers in the range [1,size of the dictionary] are mapped to single words in your dictionary. This process is commonly named 'tokenization'. In addition, we will keep the value 0 to a specific artificial word 'PADD' that will be used to account for the variable length of sentences and add to each sentence a 'START' and an 'END' word. \n",
    "\n",
    "**Code** Start by adding the three artificial words to your dictionary (list of possible tokens) and then translate every sentences to a list of integers. \n",
    "HINT: use the Python List index() method. [2 pts]\n",
    "\n",
    "**Figure** Use the violinplot to show the density of tokenized sentences length. [1pt]\n",
    "\n",
    "**Code** From this figure, select a maximum number (=maxlen) of tokens for which most of the sentences have less. Padd (and eventually truncate) all sentences with the 'PADD' token (value 0 in the integer representation) until all tokenized sentences have the same length (maxlen).\n",
    "HINT: use the pad_sequences function from keras.preprocessing.sequence [2 pts]\n",
    "\n",
    "**Code** Check that you can recover the original sentence. Randomly select two sentences from your integer and padded representation and translate them back using your dictionary. [1 pt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Generate tokens\n",
    "tokens = ['PADD', 'START', 'END'] + list(pruned.keys())\n",
    "\n",
    "## Tokenize the input\n",
    "tokenized = [[[tokens.index(word) for word in ['START'] + sentence.split(' ') + ['END']] for sentence in conversation] for conversation in filtered]\n",
    "\n",
    "## Count lengths of each sentence\n",
    "lens = Counter()\n",
    "[lens.update([len(sentence) for sentence in conversation]) for conversation in tokenized]\n",
    "\n",
    "## Plots\n",
    "_ = plt.violinplot([i for i in lens.elements()], showmeans=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0,  0,  0,  0,  1,  7, 20, 21, 22, 23, 19, 24, 25, 26, 27, 28, 29,\n",
       "        30, 19,  2], dtype=int32),\n",
       " array([ 0,  0,  0,  1, 40, 25, 45, 19, 15,  7, 46, 47, 29, 48, 35, 49, 27,\n",
       "        50,  9,  2], dtype=int32),\n",
       " array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1, 56, 25, 57, 29, 52, 14,\n",
       "        55,  9,  2], dtype=int32),\n",
       " array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  3,  4,  5,  6,  7,  8,\n",
       "        67,  9,  2], dtype=int32),\n",
       " array([ 0,  0,  0,  0,  0,  1, 10, 11, 80, 49, 81, 28, 82, 83, 56,  6,  7,\n",
       "        84, 14,  2], dtype=int32)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Visually it appears as if 20 is a reasonable cutoff\n",
    "maxlen = 20\n",
    "## Generate padded sequences; create a flat representation as well\n",
    "padded = [pad_sequences(conversation, maxlen) for conversation in tokenized]\n",
    "padded_flat = [sentence for conversation in padded for sentence in conversation]\n",
    "padded_flat[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['about the world , about animals and weather . nature stuff .',\n",
       " 'that sounds yummy . now i am hungry .']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Randomly select sentences to recover. Use tokens[i] for each integer i in the padded representation\n",
    "to_recover = [padded_flat[i] for i in np.random.randint(0, len(conversations),size=2)]\n",
    "[\" \".join([tokens[i] for i in sentence if i>2]) for sentence in to_recover]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving\n",
    "\n",
    "Now is a good time to save your data (end of processing). An example code using the pickle library is shown below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filtered_sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-752bb3ec7f7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#save\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfiltered_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'filtered_sentences' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "#save\n",
    "with open(\"data.pkl\", \"wb\") as file:\n",
    "    pickle.dump([filtered_sentences, dictionary, tokens], file)\n",
    "    \n",
    "#load\n",
    "with open(\"data.pkl\", \"rb\") as file:\n",
    "    [filtered_sentences, dictionary, tokens] = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and training generative models of language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN vs LSTM vs GRU \n",
    "\n",
    "Build, train and compare generative models of language based on RNNs with different recurrent units (SimpleRNN, GRU and LSTM). \n",
    "\n",
    "The target of the network will be to approximate the word transition probabilities Pr(word[n+1]|H[n]) with H[n]=f(word[:n]) being the hidden state of the network.  \n",
    "\n",
    "**code** You should complete the proposed model (using the Keras API rather than the Sequential model for more flexibility). Be sure to understand each line. The embedding layer allows to transform an integer to a dense vector. That would be our input to the recurrent network - each sentence is mapped to a sequence of vectors, each representing a single word. You can then design your own readout(s) and output layers. By default, use the proposed meta parameters. You can adapt them if you have more or less computing power (32 epochs should take around 30 minutes). [2 pts]\n",
    "\n",
    "**Question** How will your networks deal with the artificial word 'PADD' that you added at the end of each sentences  [2 pts]\n",
    "\n",
    "**Answer**\n",
    "\n",
    "**code** Then train three different networks with the same architecture but using different recurrent units (simpleRNN, GRU and LSTM). Save the learning history (training/validation loss and accuracy for each epoch) as well as the models. [1 pt]\n",
    "\n",
    "**Question** How can you use this network to approximate the word transition probabilities? What will be the inputs and targets of the network at each batch? Give the input/output tensor dimensions. [2 pts]\n",
    "\n",
    "**Answer**\n",
    "\n",
    "**Figure** Show the learning curves (training and validation loss) for the different recurrent units. [1 pt]\n",
    "\n",
    "**Figure** Show the learning curves (training and validation accuracy) for the different recurrent units. [1 pt]\n",
    "\n",
    "**Question:** Which recurrent unit yields the best validation accuracy? Which is the fastest learner? [1 pt]\n",
    "\n",
    "**Answer**: \n",
    "\n",
    "**Question:** Do you observe an overfitting effect? Where and for which case? Give a possible explanation. [1 pt] \n",
    "\n",
    "**Answer**: \n",
    "\n",
    "**Question:** Suggest one option modifying your dataset to decrease overfitting. [1 pt]\n",
    "\n",
    "**Answer**: \n",
    "\n",
    "**Question:** Suggest one possible option modifying your network to decrease overfitting. [1 pt]\n",
    "\n",
    "**Answer**: \n",
    "\n",
    "**Question:** Suggest one possible option modifying the training modalities to counter overfitting. [1 pt]   \n",
    "\n",
    "**Answer**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Meta-parameters\n",
    "embedding_size = 128\n",
    "hidden_size = 64\n",
    "dropout = 0.\n",
    "recurrent_dropout = 0.\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 32\n",
    "validation_split = 0.2\n",
    "\n",
    "dataset_cut = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = {}\n",
    "E = {}\n",
    "H = {}\n",
    "R = {}\n",
    "Y = {}\n",
    "models = {}\n",
    "logs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let X be the list of sentences: NxM where N is number of sentence, M is max length of pad\n",
    "X = np.array(padded_flat)\n",
    "## Let T be the one-hot encoding of X: NxMxD where D is the length of the vocabulary\n",
    "## T[n,m,d] = 1 only when mth word of the nth sentence is the dth word of the vocabulary\n",
    "T = np.zeros(X.shape + (len(tokens),))\n",
    "for i, sentence in enumerate(X):\n",
    "    for j, word in enumerate(sentence):\n",
    "        T[i,j,word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50793, 19) (50793, 19, 1010)\n"
     ]
    }
   ],
   "source": [
    "print(X[:,:-1].shape, T[:,1:].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 19)                0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 19, 128)           129280    \n",
      "_________________________________________________________________\n",
      "RNN_1 (SimpleRNN)            (None, 19, 128)           32896     \n",
      "_________________________________________________________________\n",
      "output (TimeDistributed)     (None, 19, 1010)          130290    \n",
      "=================================================================\n",
      "Total params: 292,466\n",
      "Trainable params: 292,466\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "I['RNN'] = Input(shape=(maxlen-1,), name=\"input\")\n",
    "E['RNN'] = Embedding(len(tokens), embedding_size, mask_zero=True, name=\"embedding\")(I['RNN'])\n",
    "\n",
    "#your network here\n",
    "#... Recurrent layer(s)\n",
    "H['RNN'] = [SimpleRNN(embedding_size, return_sequences=True, name='RNN_1')(E['RNN'])]\n",
    "# H['RNN'].append(Dropout(dropout)(H['RNN'][-1])) #No dropout with default parameters\n",
    "\n",
    "R['RNN'] = TimeDistributed(Dense(len(tokens), activation='softmax'), name=\"output\")(H['RNN'][-1])#... Readout\n",
    "# Y['RNN'] = Activation('softmax', name='activation')(R['RNN'])#... Output\n",
    "\n",
    "models['RNN'] = Model(inputs = [I['RNN']], outputs = [R['RNN']])\n",
    "models['RNN'].compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer=Adam(),\n",
    "    metrics=['acc'])\n",
    "models['RNN'].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40633 samples, validate on 10159 samples\n",
      "Epoch 1/32\n",
      "40633/40633 [==============================] - 82s 2ms/step - loss: 4.2032 - acc: 0.2352 - val_loss: 3.5017 - val_acc: 0.3210\n",
      "Epoch 2/32\n",
      "40633/40633 [==============================] - 82s 2ms/step - loss: 3.3491 - acc: 0.3360 - val_loss: 3.2118 - val_acc: 0.3507\n",
      "Epoch 3/32\n",
      "40633/40633 [==============================] - 85s 2ms/step - loss: 3.1461 - acc: 0.3555 - val_loss: 3.0940 - val_acc: 0.3618\n",
      "Epoch 4/32\n",
      "40633/40633 [==============================] - 85s 2ms/step - loss: 3.0405 - acc: 0.3660 - val_loss: 3.0252 - val_acc: 0.3695\n",
      "Epoch 5/32\n",
      "40633/40633 [==============================] - 86s 2ms/step - loss: 2.9714 - acc: 0.3728 - val_loss: 2.9868 - val_acc: 0.3744\n",
      "Epoch 6/32\n",
      "40633/40633 [==============================] - 86s 2ms/step - loss: 2.9213 - acc: 0.3782 - val_loss: 2.9583 - val_acc: 0.3771\n",
      "Epoch 7/32\n",
      "40633/40633 [==============================] - 86s 2ms/step - loss: 2.8818 - acc: 0.3822 - val_loss: 2.9400 - val_acc: 0.3773\n",
      "Epoch 8/32\n",
      "40633/40633 [==============================] - 86s 2ms/step - loss: 2.8487 - acc: 0.3857 - val_loss: 2.9202 - val_acc: 0.3818\n",
      "Epoch 9/32\n",
      "40633/40633 [==============================] - 87s 2ms/step - loss: 2.8209 - acc: 0.3890 - val_loss: 2.9093 - val_acc: 0.3825\n",
      "Epoch 10/32\n",
      "40633/40633 [==============================] - 86s 2ms/step - loss: 2.7965 - acc: 0.3916 - val_loss: 2.9015 - val_acc: 0.3841\n",
      "Epoch 11/32\n",
      "40633/40633 [==============================] - 85s 2ms/step - loss: 2.7752 - acc: 0.3939 - val_loss: 2.8911 - val_acc: 0.3839\n",
      "Epoch 12/32\n",
      "40633/40633 [==============================] - 86s 2ms/step - loss: 2.7549 - acc: 0.3959 - val_loss: 2.8874 - val_acc: 0.3857\n",
      "Epoch 13/32\n",
      "40633/40633 [==============================] - 87s 2ms/step - loss: 2.7377 - acc: 0.3987 - val_loss: 2.8818 - val_acc: 0.3854\n",
      "Epoch 14/32\n",
      "40633/40633 [==============================] - 87s 2ms/step - loss: 2.7216 - acc: 0.3999 - val_loss: 2.8800 - val_acc: 0.3856\n",
      "Epoch 15/32\n",
      "40633/40633 [==============================] - 87s 2ms/step - loss: 2.7053 - acc: 0.4023 - val_loss: 2.8800 - val_acc: 0.3865\n",
      "Epoch 16/32\n",
      "40633/40633 [==============================] - 89s 2ms/step - loss: 2.6922 - acc: 0.4035 - val_loss: 2.8807 - val_acc: 0.3864\n",
      "Epoch 17/32\n",
      "40633/40633 [==============================] - 93s 2ms/step - loss: 2.6787 - acc: 0.4048 - val_loss: 2.8782 - val_acc: 0.3877\n",
      "Epoch 18/32\n",
      "40633/40633 [==============================] - 88s 2ms/step - loss: 2.6666 - acc: 0.4068 - val_loss: 2.8814 - val_acc: 0.3859\n",
      "Epoch 19/32\n",
      "40633/40633 [==============================] - 83s 2ms/step - loss: 2.6545 - acc: 0.4078 - val_loss: 2.8788 - val_acc: 0.3850\n",
      "Epoch 20/32\n",
      "40633/40633 [==============================] - 97s 2ms/step - loss: 2.6436 - acc: 0.4096 - val_loss: 2.8788 - val_acc: 0.3880\n",
      "Epoch 21/32\n",
      "40633/40633 [==============================] - 86s 2ms/step - loss: 2.6336 - acc: 0.4110 - val_loss: 2.8820 - val_acc: 0.3873\n",
      "Epoch 22/32\n",
      "40633/40633 [==============================] - 91s 2ms/step - loss: 2.6234 - acc: 0.4115 - val_loss: 2.8854 - val_acc: 0.3869\n",
      "Epoch 23/32\n",
      "40633/40633 [==============================] - 92s 2ms/step - loss: 2.6143 - acc: 0.4135 - val_loss: 2.8853 - val_acc: 0.3872\n",
      "Epoch 24/32\n",
      "40633/40633 [==============================] - 102s 3ms/step - loss: 2.6052 - acc: 0.4141 - val_loss: 2.8863 - val_acc: 0.3871\n",
      "Epoch 25/32\n",
      "40633/40633 [==============================] - 100s 2ms/step - loss: 2.5963 - acc: 0.4153 - val_loss: 2.8906 - val_acc: 0.3874\n",
      "Epoch 26/32\n",
      "40633/40633 [==============================] - 100s 2ms/step - loss: 2.5885 - acc: 0.4162 - val_loss: 2.8935 - val_acc: 0.3848\n",
      "Epoch 27/32\n",
      "40633/40633 [==============================] - 94s 2ms/step - loss: 2.5804 - acc: 0.4174 - val_loss: 2.8956 - val_acc: 0.3870\n",
      "Epoch 28/32\n",
      "40633/40633 [==============================] - 88s 2ms/step - loss: 2.5725 - acc: 0.4181 - val_loss: 2.9009 - val_acc: 0.3870\n",
      "Epoch 29/32\n",
      "40633/40633 [==============================] - 84s 2ms/step - loss: 2.5656 - acc: 0.4196 - val_loss: 2.9043 - val_acc: 0.3861\n",
      "Epoch 30/32\n",
      "40633/40633 [==============================] - 85s 2ms/step - loss: 2.5592 - acc: 0.4203 - val_loss: 2.9058 - val_acc: 0.3850\n",
      "Epoch 31/32\n",
      "40633/40633 [==============================] - 86s 2ms/step - loss: 2.5521 - acc: 0.4208 - val_loss: 2.9106 - val_acc: 0.3856\n",
      "Epoch 32/32\n",
      "40633/40633 [==============================] - 85s 2ms/step - loss: 2.5455 - acc: 0.4217 - val_loss: 2.9111 - val_acc: 0.3865\n"
     ]
    }
   ],
   "source": [
    "logs['RNN'] = models['RNN'].fit({'input': X[:dataset_cut,:-1]}, {'output': T[:dataset_cut,1:]}, \n",
    "                                    epochs=epochs, \n",
    "                                    validation_split=validation_split, \n",
    "                                    batch_size=batch_size).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#save\n",
    "with open(\"RNNmodel_\"+str(embedding_size)+'_'+str(hidden_size)+\"_log.pkl\", \"wb\") as file:\n",
    "    pickle.dump(logs['RNN'], file)\n",
    "models['RNN'].save(\"RNNmodel_\"+str(embedding_size)+'_'+str(hidden_size))\n",
    "\n",
    "#load\n",
    "with open(\"RNNmodel_\"+str(embedding_size)+'_'+str(hidden_size)+\"_log.pkl\", \"rb\") as file:\n",
    "    RNNmodel_log = pickle.load(file)\n",
    "RNNmodel = load_model(\"RNNmodel_\"+str(embedding_size)+'_'+str(hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "I['LSTM'] = Input(shape=(maxlen-1,), name=\"input\")\n",
    "E['LSTM'] = Embedding(len(tokens), embedding_size, mask_zero=True, name=\"embedding\")(I['LSTM'])\n",
    "\n",
    "#your network here\n",
    "#... Recurrent layer(s)\n",
    "H['LSTM'] = [LSTM(embedding_size, return_sequences=True, name='LSTM_1')(E['LSTM'])]\n",
    "# H['LSTM'].append(Dropout(dropout)(H['LSTM'][-1]))\n",
    "\n",
    "R['LSTM'] = TimeDistributed(Dense(len(tokens), activation='softmax'), name=\"output\")(H['LSTM'][-1])#... Readout\n",
    "# Y['RNN'] = Activation('softmax', name='activation')(R['RNN'])#... Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 19)                0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 19, 128)           129280    \n",
      "_________________________________________________________________\n",
      "LSTM_1 (LSTM)                (None, 19, 128)           131584    \n",
      "_________________________________________________________________\n",
      "output (TimeDistributed)     (None, 19, 1010)          130290    \n",
      "=================================================================\n",
      "Total params: 391,154\n",
      "Trainable params: 391,154\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "models['LSTM'] = Model(inputs = [I['LSTM']], outputs = [R['LSTM']])\n",
    "models['LSTM'].compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer=Adam(),\n",
    "    metrics=['acc'])\n",
    "models['LSTM'].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40633 samples, validate on 10159 samples\n",
      "Epoch 1/32\n",
      "40633/40633 [==============================] - 107s 3ms/step - loss: 4.3927 - acc: 0.2011 - val_loss: 3.7045 - val_acc: 0.2912\n",
      "Epoch 2/32\n",
      "40633/40633 [==============================] - 106s 3ms/step - loss: 3.5229 - acc: 0.3154 - val_loss: 3.3587 - val_acc: 0.3344\n",
      "Epoch 3/32\n",
      "40633/40633 [==============================] - 107s 3ms/step - loss: 3.2862 - acc: 0.3408 - val_loss: 3.2102 - val_acc: 0.3499\n",
      "Epoch 4/32\n",
      "40633/40633 [==============================] - 117s 3ms/step - loss: 3.1601 - acc: 0.3535 - val_loss: 3.1178 - val_acc: 0.3602\n",
      "Epoch 5/32\n",
      "40633/40633 [==============================] - 108s 3ms/step - loss: 3.0768 - acc: 0.3614 - val_loss: 3.0620 - val_acc: 0.3650\n",
      "Epoch 6/32\n",
      "40633/40633 [==============================] - 107s 3ms/step - loss: 3.0161 - acc: 0.3677 - val_loss: 3.0172 - val_acc: 0.3702\n",
      "Epoch 7/32\n",
      "40633/40633 [==============================] - 110s 3ms/step - loss: 2.9685 - acc: 0.3727 - val_loss: 2.9866 - val_acc: 0.3734\n",
      "Epoch 8/32\n",
      "40633/40633 [==============================] - 124s 3ms/step - loss: 2.9294 - acc: 0.3769 - val_loss: 2.9616 - val_acc: 0.3771\n",
      "Epoch 9/32\n",
      "40633/40633 [==============================] - 129s 3ms/step - loss: 2.8967 - acc: 0.3806 - val_loss: 2.9426 - val_acc: 0.3792\n",
      "Epoch 10/32\n",
      "40633/40633 [==============================] - 125s 3ms/step - loss: 2.8682 - acc: 0.3844 - val_loss: 2.9231 - val_acc: 0.3806\n",
      "Epoch 11/32\n",
      "40633/40633 [==============================] - 135s 3ms/step - loss: 2.8423 - acc: 0.3872 - val_loss: 2.9087 - val_acc: 0.3822\n",
      "Epoch 12/32\n",
      "40633/40633 [==============================] - 114s 3ms/step - loss: 2.8199 - acc: 0.3898 - val_loss: 2.8964 - val_acc: 0.3839\n",
      "Epoch 13/32\n",
      "40633/40633 [==============================] - 116s 3ms/step - loss: 2.7988 - acc: 0.3921 - val_loss: 2.8851 - val_acc: 0.3862\n",
      "Epoch 14/32\n",
      "40633/40633 [==============================] - 111s 3ms/step - loss: 2.7798 - acc: 0.3945 - val_loss: 2.8764 - val_acc: 0.3868\n",
      "Epoch 15/32\n",
      "40633/40633 [==============================] - 131s 3ms/step - loss: 2.7619 - acc: 0.3963 - val_loss: 2.8716 - val_acc: 0.3887\n",
      "Epoch 16/32\n",
      "40633/40633 [==============================] - 127s 3ms/step - loss: 2.7453 - acc: 0.3985 - val_loss: 2.8631 - val_acc: 0.3886\n",
      "Epoch 17/32\n",
      "40633/40633 [==============================] - 126s 3ms/step - loss: 2.7298 - acc: 0.4000 - val_loss: 2.8603 - val_acc: 0.3888\n",
      "Epoch 18/32\n",
      "40633/40633 [==============================] - 111s 3ms/step - loss: 2.7152 - acc: 0.4019 - val_loss: 2.8552 - val_acc: 0.3901\n",
      "Epoch 19/32\n",
      "40633/40633 [==============================] - 109s 3ms/step - loss: 2.7007 - acc: 0.4037 - val_loss: 2.8538 - val_acc: 0.3899\n",
      "Epoch 20/32\n",
      "40633/40633 [==============================] - 121s 3ms/step - loss: 2.6870 - acc: 0.4055 - val_loss: 2.8511 - val_acc: 0.3906\n",
      "Epoch 21/32\n",
      "40633/40633 [==============================] - 126s 3ms/step - loss: 2.6750 - acc: 0.4067 - val_loss: 2.8449 - val_acc: 0.3922\n",
      "Epoch 22/32\n",
      "40633/40633 [==============================] - 121s 3ms/step - loss: 2.6623 - acc: 0.4082 - val_loss: 2.8427 - val_acc: 0.3922\n",
      "Epoch 23/32\n",
      "40633/40633 [==============================] - 109s 3ms/step - loss: 2.6507 - acc: 0.4100 - val_loss: 2.8436 - val_acc: 0.3919\n",
      "Epoch 24/32\n",
      "40633/40633 [==============================] - 123s 3ms/step - loss: 2.6391 - acc: 0.4113 - val_loss: 2.8432 - val_acc: 0.3934\n",
      "Epoch 25/32\n",
      "40633/40633 [==============================] - 123s 3ms/step - loss: 2.6283 - acc: 0.4127 - val_loss: 2.8401 - val_acc: 0.3929\n",
      "Epoch 26/32\n",
      "40633/40633 [==============================] - 130s 3ms/step - loss: 2.6177 - acc: 0.4140 - val_loss: 2.8401 - val_acc: 0.3922\n",
      "Epoch 27/32\n",
      "40633/40633 [==============================] - 118s 3ms/step - loss: 2.6071 - acc: 0.4158 - val_loss: 2.8397 - val_acc: 0.3925\n",
      "Epoch 28/32\n",
      "40633/40633 [==============================] - 114s 3ms/step - loss: 2.5973 - acc: 0.4168 - val_loss: 2.8421 - val_acc: 0.3913\n",
      "Epoch 29/32\n",
      "40633/40633 [==============================] - 130s 3ms/step - loss: 2.5875 - acc: 0.4186 - val_loss: 2.8408 - val_acc: 0.3932\n",
      "Epoch 30/32\n",
      "40633/40633 [==============================] - 128s 3ms/step - loss: 2.5784 - acc: 0.4197 - val_loss: 2.8408 - val_acc: 0.3929\n",
      "Epoch 31/32\n",
      "40633/40633 [==============================] - 128s 3ms/step - loss: 2.5693 - acc: 0.4208 - val_loss: 2.8407 - val_acc: 0.3929\n",
      "Epoch 32/32\n",
      "40633/40633 [==============================] - 118s 3ms/step - loss: 2.5602 - acc: 0.4218 - val_loss: 2.8425 - val_acc: 0.3922\n"
     ]
    }
   ],
   "source": [
    "logs['LSTM'] = models['LSTM'].fit({'input': X[:dataset_cut,:-1]}, {'output': T[:dataset_cut,1:]}, \n",
    "                                    epochs=epochs, \n",
    "                                    validation_split=validation_split, \n",
    "                                    batch_size=batch_size).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save\n",
    "with open(\"LSTMmodel_\"+str(embedding_size)+'_'+str(hidden_size)+\"_log.pkl\", \"wb\") as file:\n",
    "    pickle.dump(logs['LSTM'], file)\n",
    "models['LSTM'].save(\"LSTMmodel_\"+str(embedding_size)+'_'+str(hidden_size))\n",
    "\n",
    "#load\n",
    "with open(\"LSTMmodel_\"+str(embedding_size)+'_'+str(hidden_size)+\"_log.pkl\", \"rb\") as file:\n",
    "    LSTMmodel_log = pickle.load(file)\n",
    "LSTMmodel = load_model(\"LSTMmodel_\"+str(embedding_size)+'_'+str(hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 19)                0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 19, 128)           129280    \n",
      "_________________________________________________________________\n",
      "GRU_1 (GRU)                  (None, 19, 128)           98688     \n",
      "_________________________________________________________________\n",
      "output (TimeDistributed)     (None, 19, 1010)          130290    \n",
      "=================================================================\n",
      "Total params: 358,258\n",
      "Trainable params: 358,258\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "I['GRU'] = Input(shape=(maxlen-1,), name=\"input\")\n",
    "E['GRU'] = Embedding(len(tokens), embedding_size, mask_zero=True, name=\"embedding\")(I['GRU'])\n",
    "\n",
    "#your network here\n",
    "#... Recurrent layer(s)\n",
    "H['GRU'] = [GRU(embedding_size, return_sequences=True, name='GRU_1')(E['GRU'])]\n",
    "# H['LSTM'].append(Dropout(dropout)(H['LSTM'][-1]))\n",
    "\n",
    "R['GRU'] = TimeDistributed(Dense(len(tokens), activation='softmax'), name=\"output\")(H['GRU'][-1])#... Readout\n",
    "# Y['RNN'] = Activation('softmax', name='activation')(R['RNN'])#... Output\n",
    "\n",
    "models['GRU'] = Model(inputs = [I['GRU']], outputs = [R['GRU']])\n",
    "models['GRU'].compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer=Adam(),\n",
    "    metrics=['acc'])\n",
    "models['GRU'].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40633 samples, validate on 10159 samples\n",
      "Epoch 1/32\n",
      "40633/40633 [==============================] - 107s 3ms/step - loss: 4.4132 - acc: 0.2076 - val_loss: 3.5909 - val_acc: 0.3118\n",
      "Epoch 2/32\n",
      "40633/40633 [==============================] - 104s 3ms/step - loss: 3.3812 - acc: 0.3324 - val_loss: 3.2077 - val_acc: 0.3518\n",
      "Epoch 3/32\n",
      "40633/40633 [==============================] - 101s 2ms/step - loss: 3.1306 - acc: 0.3575 - val_loss: 3.0648 - val_acc: 0.3658\n",
      "Epoch 4/32\n",
      "40633/40633 [==============================] - 101s 2ms/step - loss: 3.0062 - acc: 0.3711 - val_loss: 2.9835 - val_acc: 0.3761\n",
      "Epoch 5/32\n",
      "40633/40633 [==============================] - 102s 3ms/step - loss: 2.9260 - acc: 0.3802 - val_loss: 2.9349 - val_acc: 0.3816\n",
      "Epoch 6/32\n",
      "40633/40633 [==============================] - 102s 3ms/step - loss: 2.8680 - acc: 0.3863 - val_loss: 2.9007 - val_acc: 0.3850\n",
      "Epoch 7/32\n",
      "40633/40633 [==============================] - 102s 3ms/step - loss: 2.8211 - acc: 0.3913 - val_loss: 2.8763 - val_acc: 0.3876\n",
      "Epoch 8/32\n",
      "40633/40633 [==============================] - 115s 3ms/step - loss: 2.7835 - acc: 0.3957 - val_loss: 2.8585 - val_acc: 0.3896\n",
      "Epoch 9/32\n",
      "40633/40633 [==============================] - 108s 3ms/step - loss: 2.7502 - acc: 0.3991 - val_loss: 2.8450 - val_acc: 0.3918\n",
      "Epoch 10/32\n",
      "40633/40633 [==============================] - 117s 3ms/step - loss: 2.7219 - acc: 0.4024 - val_loss: 2.8336 - val_acc: 0.3931\n",
      "Epoch 11/32\n",
      "40633/40633 [==============================] - 123s 3ms/step - loss: 2.6962 - acc: 0.4053 - val_loss: 2.8307 - val_acc: 0.3924\n",
      "Epoch 12/32\n",
      "40633/40633 [==============================] - 129s 3ms/step - loss: 2.6731 - acc: 0.4081 - val_loss: 2.8229 - val_acc: 0.3945\n",
      "Epoch 13/32\n",
      "40633/40633 [==============================] - 118s 3ms/step - loss: 2.6526 - acc: 0.4106 - val_loss: 2.8221 - val_acc: 0.3952\n",
      "Epoch 14/32\n",
      "40633/40633 [==============================] - 119s 3ms/step - loss: 2.6330 - acc: 0.4132 - val_loss: 2.8166 - val_acc: 0.3971\n",
      "Epoch 15/32\n",
      "40633/40633 [==============================] - 137s 3ms/step - loss: 2.6149 - acc: 0.4149 - val_loss: 2.8184 - val_acc: 0.3948\n",
      "Epoch 16/32\n",
      "40633/40633 [==============================] - 127s 3ms/step - loss: 2.5979 - acc: 0.4172 - val_loss: 2.8197 - val_acc: 0.3944\n",
      "Epoch 17/32\n",
      "40633/40633 [==============================] - 121s 3ms/step - loss: 2.5819 - acc: 0.4194 - val_loss: 2.8211 - val_acc: 0.3960\n",
      "Epoch 18/32\n",
      "40633/40633 [==============================] - 118s 3ms/step - loss: 2.5662 - acc: 0.4214 - val_loss: 2.8234 - val_acc: 0.3956\n",
      "Epoch 19/32\n",
      "40633/40633 [==============================] - 129s 3ms/step - loss: 2.5520 - acc: 0.4235 - val_loss: 2.8249 - val_acc: 0.3942\n",
      "Epoch 20/32\n",
      "40633/40633 [==============================] - 136s 3ms/step - loss: 2.5382 - acc: 0.4246 - val_loss: 2.8277 - val_acc: 0.3971\n",
      "Epoch 21/32\n",
      "40633/40633 [==============================] - 130s 3ms/step - loss: 2.5247 - acc: 0.4266 - val_loss: 2.8305 - val_acc: 0.3960\n",
      "Epoch 22/32\n",
      "40633/40633 [==============================] - 128s 3ms/step - loss: 2.5120 - acc: 0.4286 - val_loss: 2.8345 - val_acc: 0.3963\n",
      "Epoch 23/32\n",
      "40633/40633 [==============================] - 129s 3ms/step - loss: 2.4996 - acc: 0.4302 - val_loss: 2.8419 - val_acc: 0.3933\n",
      "Epoch 24/32\n",
      "40633/40633 [==============================] - 125s 3ms/step - loss: 2.4874 - acc: 0.4320 - val_loss: 2.8431 - val_acc: 0.3962\n",
      "Epoch 25/32\n",
      "40633/40633 [==============================] - 130s 3ms/step - loss: 2.4761 - acc: 0.4334 - val_loss: 2.8510 - val_acc: 0.3950\n",
      "Epoch 26/32\n",
      "40633/40633 [==============================] - 123s 3ms/step - loss: 2.4651 - acc: 0.4350 - val_loss: 2.8573 - val_acc: 0.3930\n",
      "Epoch 27/32\n",
      "40633/40633 [==============================] - 123s 3ms/step - loss: 2.4543 - acc: 0.4363 - val_loss: 2.8597 - val_acc: 0.3946\n",
      "Epoch 28/32\n",
      "40633/40633 [==============================] - 124s 3ms/step - loss: 2.4435 - acc: 0.4382 - val_loss: 2.8674 - val_acc: 0.3932\n",
      "Epoch 29/32\n",
      "40633/40633 [==============================] - 123s 3ms/step - loss: 2.4338 - acc: 0.4398 - val_loss: 2.8732 - val_acc: 0.3929\n",
      "Epoch 30/32\n",
      "40633/40633 [==============================] - 136s 3ms/step - loss: 2.4235 - acc: 0.4416 - val_loss: 2.8803 - val_acc: 0.3909\n",
      "Epoch 31/32\n",
      "40633/40633 [==============================] - 111s 3ms/step - loss: 2.4140 - acc: 0.4431 - val_loss: 2.8859 - val_acc: 0.3912\n",
      "Epoch 32/32\n",
      "40633/40633 [==============================] - 110s 3ms/step - loss: 2.4049 - acc: 0.4448 - val_loss: 2.8939 - val_acc: 0.3909\n"
     ]
    }
   ],
   "source": [
    "logs['GRU'] = models['GRU'].fit({'input': X[:dataset_cut,:-1]}, {'output': T[:dataset_cut,1:]}, \n",
    "                                    epochs=epochs, \n",
    "                                    validation_split=validation_split, \n",
    "                                    batch_size=batch_size).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save\n",
    "with open(\"GRUmodel_\"+str(embedding_size)+'_'+str(hidden_size)+\"_log.pkl\", \"wb\") as file:\n",
    "    pickle.dump(logs['GRU'], file)\n",
    "models['GRU'].save(\"GRUmodel_\"+str(embedding_size)+'_'+str(hidden_size))\n",
    "\n",
    "#load\n",
    "with open(\"GRUmodel_\"+str(embedding_size)+'_'+str(hidden_size)+\"_log.pkl\", \"rb\") as file:\n",
    "    GRUmodel_log = pickle.load(file)\n",
    "GRUmodel = load_model(\"GRUmodel_\"+str(embedding_size)+'_'+str(hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next(prefix, model, tokens):\n",
    "    maxlen = model.input_shape[-1]\n",
    "    unpadded = [tokens.index(i) for i in prefix.split(' ')]\n",
    "    padded = pad_sequences([unpadded], maxlen)\n",
    "    pred = tokens[np.argmax(model.predict(padded)[0][-1])]\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = GRUmodel.predict(X[0:20, :-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('PADD', '')\n",
      "('PADD', ',')\n",
      "('PADD', ',')\n",
      "('PADD', ',')\n",
      "('PADD', ',')\n",
      "('PADD', ',')\n",
      "('START', ',')\n",
      "('i', 'i')\n",
      "('also', 'am')\n",
      "('cook', 'like')\n",
      "(',', 'for')\n",
      "('and', 'and')\n",
      "('i', 'i')\n",
      "('ride', 'love')\n",
      "('my', 'my')\n",
      "('bike', 'bike')\n",
      "('to', 'to')\n",
      "('work', 'work')\n",
      "('.', '.')\n"
     ]
    }
   ],
   "source": [
    "for i in zip([tokens[i] for i in X[15,:-1]], [\"\"]+[tokens[np.argmax(i)] for i in preds[15]]):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal network size\n",
    "\n",
    "Compare the learning curves for three networks with 64 (previous exercise), 128 and 256 GRUs (single layer) and one with two hidden layers of 64 GRUs. \n",
    "\n",
    "**Code** Build and train the networks. Apply EarlyStopping (monitor='val_acc', min_delta=0.001, patience=2). Use transfer learning, do not train from scratch your embedding layer, rather re-use the embedding layer from your best performing network in the last exercise. [4 pts]\n",
    "\n",
    "**Figure** Show the learning curves (training and validation loss) for the four models. [1 pt]\n",
    "\n",
    "**Figure** Show the learning curves (training and validation accuracy) for the four models. [1 pt]\n",
    "\n",
    "**Question** List and briefly explain the differences in the learning curves for the different models? [2 pts]\n",
    "\n",
    "**Answer**\n",
    "\n",
    "**Question** What effect had EarlyStopping? Give one advantage and one drawback. [2 pts]\n",
    "\n",
    "**Answer**\n",
    "\n",
    "**Question** What is your best model? Why? [1 pt]\n",
    "\n",
    "**Answer**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate sentences\n",
    "\n",
    "Now you will generate new sentences from your best performing model.\n",
    "\n",
    "**Code** To achieve that, use the provided sample function below to generate new sentences from your model. You should start by constructing a sentence that starts with the 'START' artificial word and all other words being the 'PADD' artificial word. Then sample the first word from the corresponding probabilities given by your model. Add this word to the sentence and continue like this until you sample the 'END' artificial word or the maximum sentence length. [2 pts]\n",
    "\n",
    "**Code** Generate 10 sentences for different sampling temperature in [0., 0.25, 0.5, 0.75, 1., 1.5., 2.]. [1 pt]\n",
    "\n",
    "**7 Figures** For each temperature, use matplotlib imshow to plot the probablities of every word in one generated sentence (and only these words) at each time step. y-axis should be the words that are present in the sentence. x-axis the timesteps and the imshow value the probabilities given by the model for all words in the sentence at each timestep. Use the a colormap where 0 is white, e.g. cmap='Greys'. [2 pts]\n",
    "\n",
    "**Code** Finally, seed your model with two different beginnings of max 4 words and let it generate 10 possible continuations (use sampling temperature of 1.). [2 pts]\n",
    "\n",
    "**Question** What is the effect of sampling temperature on the generated sentences? [1 pt]\n",
    "\n",
    "**Answer**\n",
    "\n",
    "**Question** In terms of sampling a probability distribution, what does a sampling temperature of 0 corresponds to? [1 pt] \n",
    "\n",
    "**Answer**\n",
    "\n",
    "**Question** In terms of sampling a probability distribution, what does a sampling temperature of 1. corresponds to? [1 pt] \n",
    "\n",
    "**Answer**\n",
    "\n",
    "**Question** In terms of sampling a probability distribution, what does a very high sampling temperature corresponds to? [1 pt]\n",
    "\n",
    "**Answer**\n",
    "\n",
    "**Question** Based on the plotted word probabilities, explain how a sentence is generated. [2 pts]\n",
    "\n",
    "**Answer**\n",
    "\n",
    "**Question** Do you observe timesteps with more than one word with non-zero probability? How do these probable words relate in terms of language? [1 pt]\n",
    "\n",
    "**Answer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.):\n",
    "    # helper function to sample an index from a probability array\n",
    "    if temperature == 0.:\n",
    "        return np.argmax(preds)\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Word embedding visualization\n",
    "\n",
    "Here, you are asked to visualize the embedding layer. \n",
    "\n",
    "**Code** To do that, project in 2D the embedding vectors for different words. Use t-SNE, a projection that conserve the neighborhood relationships between vectors. HINT: Build a Keras model that takes as input a list of words and outputs a list of vector embeddings as learned by your best performing model. Use t-SNE dimensionality reduction (from sklearn.manifold import TSNE). [2 pts]\n",
    "\n",
    "**Figure** Plot the projection of the first 200 most frequent words in a 2D plot. On the plot, write the words. [2 pt] \n",
    "\n",
    "**Question** Do you observe clusters of words with similar meaning or role in language? Report three of them here. [1 pt]\n",
    "\n",
    "**Answer**\n",
    "\n",
    "**Question** Why is having similar vector representation for similar words a good approach for such models? Explain using the example clusters from before and argue in terms of prediction accuracy and/or generalization. [2 pts]\n",
    "\n",
    "**Answer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot\n",
    "\n",
    "Finally, you will construct a model with which you can chat. The network will take as input a sentence and output a response.\n",
    "\n",
    "**Code** For that, you should go back to your original data and construct a new dataset containing pairs of sentences, where each pair is a sentence and its answer. Be careful to not include any pair of sentences that contains words not present in your filtered dictionary. [2 pts]\n",
    "\n",
    "**Code** You should then tokenize, padd, truncate each sentence. Only the answers need the 'START' and 'END' artificial words. [1 pt]\n",
    "\n",
    "We provide you with a possible model, you are welcome to change it. This model uses an LSTM layer to encode the first sentence (the context). The final state of this LSTM layer is transfered to initialize the state of a decoder LSTM layer from which the answer sentence will be generated. \n",
    "\n",
    "**Code** Train your chatbot model on your dataset. [1 pt]\n",
    "\n",
    "**Code** Adapt your sentence generation code from before so that you can generate an answer given a context sentence from your model. [2 pts] \n",
    "\n",
    "**Code** After training, randomly select 10 context-answers pairs from your data and show both the real answer (the one from the data) and the generated one for two different sampling temperatures (e.g. 0.5 and 1.0). [2 pts]\n",
    "\n",
    "**Question** How similar are the generated answers and the real ones? Does your model provide probable answers (given the dataset)? Report here one good and one bad example. [2 pts]\n",
    "\n",
    "**Answer**\n",
    "\n",
    "**Question** Which sampling temperature gives better answers? why? [2 pts]\n",
    "\n",
    "**Answer**\n",
    "\n",
    "**Question** Would it be good if your model was able to reproduce exactly each real answer? Why? [1 pt]\n",
    "\n",
    "**Answer**\n",
    "\n",
    "**Code** Entertain yourself with your model. Write some code to chat with your bot, let it discuss with itself, ... be creative! [2 **bonus** pts]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "context = Input(shape=(maxlen-2,), name=\"input_context\")\n",
    "shared_embedding = E['GRU']\n",
    "context_embedding = shared_embedding(context)\n",
    "\n",
    "encoder_y, encoder_h, encoder_c = LSTM(hidden_size, \n",
    "            return_sequences=False,\n",
    "            return_state=True,\n",
    "            stateful=False,\n",
    "            dropout=dropout,\n",
    "            recurrent_dropout=recurrent_dropout,\n",
    "            go_backwards=True,\n",
    "            name=\"encoder\")(context_embedding)\n",
    "\n",
    "answer = Input(shape=(maxlen-1,), name=\"input_answer\")\n",
    "answer_embedding = shared_embedding(answer)\n",
    "\n",
    "decoder_input = answer_embedding\n",
    "decoder = LSTM(hidden_size, \n",
    "            return_sequences=True,\n",
    "            stateful=False,\n",
    "            dropout=dropout,\n",
    "            recurrent_dropout=recurrent_dropout,\n",
    "            name=\"decoder\")(answer_embedding, initial_state=[encoder_h, encoder_c])\n",
    "# decoder2 = LSTM(hidden_size, \n",
    "#             return_sequences=True,\n",
    "#             stateful=False,\n",
    "#             dropout=dropout,\n",
    "#             recurrent_dropout=recurrent_dropout,\n",
    "#             name=\"decoder2\")(decoder)\n",
    "\n",
    "R = TimeDistributed(Dense(embedding_size, activation='relu'), name='readout')(decoder)\n",
    "Y = TimeDistributed(Dense(len(tokens), activation='softmax'), name='output')(R)\n",
    "\n",
    "Chatbot = Model(inputs = [context, answer], outputs = [Y])\n",
    "Chatbot.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer=Adam(),\n",
    "    metrics=['acc'])\n",
    "Chatbot.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs456env",
   "language": "python",
   "name": "cs456env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
